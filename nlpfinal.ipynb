{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":231755,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":197671,"modelId":219494}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Dot product between two words using word2vec embeddings \n","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\n\n# Load Pre-trained Word2Vec Model\ndef load_word2vec_model(path):\n    return KeyedVectors.load_word2vec_format(path, binary=True)\n\n# Get Word Vector\ndef get_word_vector(word, model):\n    if word in model.key_to_index:\n        return model[word]\n    else:\n        raise ValueError(f\"Word '{word}' not found in the vocabulary!\")\n\n# Compute Dot Product\ndef dot_product(vec1, vec2):\n    return np.dot(vec1, vec2)\n\n# Display Embeddings\ndef display_embeddings(word, vector):\n    print(f\"Embedding for '{word}':\\n{vector}\")\n\n# Main Script\nmodel_path = \"/kaggle/input/word2vec/pytorch/default/1/GoogleNews-vectors-negative300.bin\"  # Update with actual path\nmodel = load_word2vec_model(model_path)\n\nword1 = input(\"Enter the first word: \").strip()\nword2 = input(\"Enter the second word: \").strip()\n\ntry:\n    vec1 = get_word_vector(word1, model)\n    vec2 = get_word_vector(word2, model)\n    result = dot_product(vec1, vec2)\n    print(f\"Dot product between '{word1}' and '{word2}': {result:.4f}\\n\")\n   # display_embeddings(word1, vec1)\n   #  display_embeddings(word2, vec2)\nexcept ValueError as e:\n    print(e)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T12:02:17.235973Z","iopub.execute_input":"2025-01-16T12:02:17.236266Z","iopub.status.idle":"2025-01-16T12:03:08.624987Z","shell.execute_reply.started":"2025-01-16T12:02:17.236244Z","shell.execute_reply":"2025-01-16T12:03:08.624092Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the first word:  king\nEnter the second word:  queen\n"},{"name":"stdout","text":"Dot product between 'king' and 'queen': 5.7224\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# 2.Cosine similarity between two words using word2vec embeddings \n","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n# Load pre-trained Word2Vec model (e.g., Google News vectors)\nmodel = KeyedVectors.load_word2vec_format('/kaggle/input/word2vec/pytorch/default/1/GoogleNews-vectors-negative300.bin', binary=True)\nfrom numpy import dot\nfrom numpy.linalg import norm\n\nword1 = input(\"Enter the first word: \").strip()\nword2 = input(\"Enter the second word: \").strip()\n\nvector1 = model[word1]\nvector2 = model[word2]\n\ncosine_similarity = dot(vector1, vector2) / (norm(vector1) * norm(vector2))\nprint(f\"Cosine Similarity between '{word1}' and '{word2}': {cosine_similarity}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T12:03:13.845013Z","iopub.execute_input":"2025-01-16T12:03:13.845326Z","iopub.status.idle":"2025-01-16T12:05:28.401556Z","shell.execute_reply.started":"2025-01-16T12:03:13.845300Z","shell.execute_reply":"2025-01-16T12:05:28.400671Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the first word:  king\nEnter the second word:  queen\n"},{"name":"stdout","text":"Cosine Similarity between 'king' and 'queen': 0.6510956883430481\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# 3.Sentiment analysis on sample comments(positive/negative) using word2vec embeddings and two layer simple neural network ","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load Pre-trained Word2Vec Model\ndef load_word2vec_model(path):\n    return KeyedVectors.load_word2vec_format(path, binary=True)\n\n# Get Sentence Vector\ndef get_sentence_vector(sentence, model):\n    words = sentence.split()\n    word_vectors = [model[word] for word in words if word in model.key_to_index]\n    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n\n# Perform Sentiment Analysis using Neural Network\ndef sentiment_analysis_nn(texts, labels, model):\n    X = np.array([get_sentence_vector(text, model) for text in texts])\n    le = LabelEncoder()\n    y = le.fit_transform(labels)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    nn_model = Sequential([\n        Dense(128, activation='relu', input_shape=(X.shape[1],)),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    nn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n    \n    return nn_model\n\n# Display Word Embeddings\ndef display_embeddings(text, model):\n    words = text.split()\n    for word in words:\n        if word in model.key_to_index:\n            print(f\"Embedding for '{word}':\\n{model[word]}\\n\")\n\n# Main Script\nmodel_path = \"/kaggle/input/word2vec/pytorch/default/1/GoogleNews-vectors-negative300.bin\"  # Update with actual path\nword2vec_model = load_word2vec_model(model_path)\n\nsample_texts = [\"I love this movie\", \"This product is terrible\", \"Amazing experience\", \"Not good at all\"]\nsample_labels = [\"positive\", \"negative\", \"positive\", \"negative\"]\n\n# Perform Sentiment Analysis\nnn_model = sentiment_analysis_nn(sample_texts, sample_labels, word2vec_model)\n\n# Comment out the lines printing embeddings\n# for text in sample_texts:\n#     print(f\"Text: {text}\")\n#     display_embeddings(text, word2vec_model)\n#     print(\"--------------------------------------------------\")\n\n# Take User Sentence for Prediction\nuser_sentence = input(\"Enter a sentence to analyze sentiment: \").strip()\nuser_vector = get_sentence_vector(user_sentence, word2vec_model).reshape(1, -1)\nprediction = nn_model.predict(user_vector)\nsentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n\nprint(f\"Predicted sentiment for '{user_sentence}': {sentiment}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T12:15:11.931359Z","iopub.execute_input":"2025-01-16T12:15:11.931694Z","iopub.status.idle":"2025-01-16T12:16:25.327380Z","shell.execute_reply.started":"2025-01-16T12:15:11.931667Z","shell.execute_reply":"2025-01-16T12:16:25.326480Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.3333 - loss: 0.6898 - val_accuracy: 0.0000e+00 - val_loss: 0.7231\nEpoch 2/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.6468 - val_accuracy: 0.0000e+00 - val_loss: 0.7380\nEpoch 3/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.6146 - val_accuracy: 0.0000e+00 - val_loss: 0.7527\nEpoch 4/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.5885 - val_accuracy: 0.0000e+00 - val_loss: 0.7669\nEpoch 5/5\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.5625 - val_accuracy: 0.0000e+00 - val_loss: 0.7773\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a sentence to analyze sentiment:  That movie is so good\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\nPredicted sentiment for 'That movie is so good': positive\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}